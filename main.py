from comet_ml import Experimentimport torchimport torch.optim as optimimport torch.nn as nnimport torchvisionfrom torchvision import transformsfrom functools import partialfrom tqdm import tqdmfrom pathlib import Pathfrom modular_cnn import ModularCNN, make_layersfrom torchsummary import summaryfrom torchutils import pickle_loader, get_num_correct, get_train_val_idx, DatasetFolderWithPaths, get_mistakes, online_mean_and_std, calc_accuracytorch.backends.cudnn.benchmark=Trueprint('CUDA available:', torch.cuda.is_available())print('CUDA enabled:', torch.backends.cudnn.enabled)# torch.cuda.empty_cache()log_data = FalseHP1 = {"learning_rate": 0.00001               ,"n_epochs": 100               ,"batch_size": 64               ,"num_workers": 3               ,"normalized_data": True               ,"stratified": True               ,"max_frames": 10               ,"dataset": "5frame_steps"               ,"resolution": 100               ,"adaptive_pool": (4, 4, 4)               ,"features": [8, 'M', 16, 'M', 32, 'M', 64, 'M', 128, 'M']               ,"classifier": [0.7, 256, 0.7, 256]                }for hyper_params in [HP1]:    model = ModularCNN(make_layers(hyper_params["features"], batch_norm=True), classifier = hyper_params["classifier"], adaptive_pool=hyper_params["adaptive_pool"])    if torch.cuda.is_available():        model = model.cuda()    if torch.cuda.device_count() > 1:      print("Let's use", torch.cuda.device_count(), "GPUs!")      # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs      model = nn.DataParallel(model)    # Log number of parameters    hyper_params['trainable_params'] = sum(p.numel() for p in model.parameters())    print('N_trainable_params:', hyper_params['trainable_params'])    data_transforms = transforms.Compose([        transforms.ToTensor()        ,transforms.Normalize([0.21308169, 0.2133352,  0.21363254, 0.21361411, 0.21371935, 0.21371628, 0.21360689, 0.21336799, 0.21276978, 0.2121863 ],                              [0.21369012, 0.21371147, 0.21385933, 0.2138414,  0.21385933, 0.21388142, 0.21384666, 0.21386346, 0.2137403,  0.21360974])    ])    # ROOT_PATH = str("/home/ido/data/" + hyper_params['dataset'])    ROOT_PATH = str('/Users/idofarhi/Documents/Thesis/Data/frames/' + hyper_params['dataset'])    master_data_set = DatasetFolderWithPaths(ROOT_PATH                                    # , transform = data_transforms                                    , loader = partial(pickle_loader, max_frames = hyper_params['max_frames'])                                    , extensions = '.pickle'                                    )    train_idx, val_idx = get_train_val_idx(master_data_set)    train_set = torch.utils.data.Subset(master_data_set, train_idx)    val_set = torch.utils.data.Subset(master_data_set, val_idx)    train_loader = torch.utils.data.DataLoader(train_set                                         , batch_size=hyper_params['batch_size']                                         , shuffle=True                                         # ,batch_sampler =  # TODO: add stratified sampling                                         , num_workers=hyper_params['num_workers']                                         , drop_last=False                                         )    # online_mean_and_std(train_loader)    val_loader = torch.utils.data.DataLoader(val_set                                         , batch_size=hyper_params['batch_size']                                         , shuffle=True                                         # ,batch_sampler =  # TODO: add stratified sampling                                         , num_workers=hyper_params['num_workers']                                         , drop_last=False                                         )    optimizer = optim.Adam(model.parameters(), lr=hyper_params['learning_rate'])    criterion = nn.CrossEntropyLoss()    log_number_train = log_number_val = 0    if log_data:        # Comet ML experiment        experiment = Experiment(api_key="BEnSW6NdCjUCZsWIto0yhxts1" ,project_name="thesis" ,workspace="idodox")        experiment.log_parameters(hyper_params)    summary(model, (1, hyper_params["max_frames"], hyper_params["resolution"], hyper_params["resolution"]))    for epoch in tqdm(range(hyper_params["n_epochs"])):        total_train_loss = 0        total_train_correct = 0        incorrect_classifications_train = []        incorrect_classifications_val = []        epoch_classifications_train = []        epoch_classifications_val = []        model.train()        for batch_number, (images, labels, paths) in enumerate(train_loader):            images = torch.unsqueeze(images, 1).double()  # added channel dimensions (grayscale)            labels = labels.long()            if torch.cuda.is_available():                images, labels = images.cuda(), labels.cuda()            optimizer.zero_grad() # Whenever pytorch calculates gradients it always adds it to whatever it has, so we need to reset it each batch.            preds = model(images) # Pass Batch            loss = criterion(preds, labels) # Calculate Loss            total_train_loss += loss.item()            loss.backward() # Calculate Gradients - the gradient is the direction we need to move towards the loss function minimum (LR will tell us how far to step)            optimizer.step() # Update Weights - the optimizer is able to update the weights because we passed it the weights as an argument in line 4.            num_correct = get_num_correct(preds, labels)            total_train_correct += num_correct            if log_data:                experiment.log_metric("Train batch accuracy", num_correct/len(labels)*100, step = log_number_train)                experiment.log_metric("Train batch CrossEntropyLoss", loss.item(), step = log_number_train)            log_number_train += 1            # print('Train: Batch number:', batch_number, 'Num correct:', num_correct, 'Accuracy:', "{:.2%}".format(num_correct/len(labels)), 'Loss:', loss.item())            incorrect_classifications_train.append(get_mistakes(preds, labels, paths))            for prediction in zip(preds, labels, paths):                epoch_classifications_train.append(prediction)        epoch_accuracy = calc_accuracy(epoch_classifications_train)        if log_data:            experiment.log_metric("Train epoch accuracy", epoch_accuracy, step = epoch)            experiment.log_metric("Train epoch CrossEntropyLoss", total_train_loss, step = epoch)        print('Train: Epoch:', epoch, 'num correct:', total_train_correct, 'Accuracy:', str(epoch_accuracy) + '%', 'Batch loss:', total_train_loss)        total_val_loss = 0        total_val_correct = 0        model.eval()        with torch.no_grad():            for batch_number, (images, labels, paths) in enumerate(val_loader):                images = torch.unsqueeze(images, 1).double()  # added channel dimensions (grayscale)                labels = labels.long()                if torch.cuda.is_available():                    images, labels = images.cuda(), labels.cuda()                preds = model(images)  # Pass Batch                loss = criterion(preds, labels)  # Calculate Loss                total_val_loss += loss.item()                num_correct = get_num_correct(preds, labels)                total_val_correct += num_correct                if log_data:                    experiment.log_metric("Val batch accuracy", num_correct / len(labels) * 100, step=log_number_val)                    experiment.log_metric("Val batch CrossEntropyLoss", loss.item(), step=log_number_val)                log_number_val += 1                # print('Val: Batch number:', batch_number, 'Num correct:', num_correct, 'Accuracy:', "{:.2%}".format(num_correct / len(labels)), 'Loss:', loss.item())                # print_mistakes(preds, labels, paths)                incorrect_classifications_val.append(get_mistakes(preds, labels, paths))                for prediction in zip(preds, labels, paths):                    epoch_classifications_val.append(prediction)            epoch_accuracy = calc_accuracy(epoch_classifications_val)            if log_data:                experiment.log_metric("Val epoch accuracy", epoch_accuracy, step=epoch)                experiment.log_metric("Val epoch CrossEntropyLoss", total_val_loss, step=epoch)            print('Val Epoch:', epoch, 'num correct:', total_val_correct, 'Accuracy:', str(epoch_accuracy) + '%' , 'Batch loss:', total_val_loss)        if epoch >= hyper_params['n_epochs']-1:            print('TRAIN MISCLASSIFICATIONS:')            print(incorrect_classifications_train)            print('TEST MISCLASSIFICATIONS:')            print(incorrect_classifications_val)