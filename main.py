from comet_ml import Experimentimport torchimport torch.optim as optimimport torch.nn as nnimport torchvisionfrom torchvision import transformsfrom functools import partialfrom tqdm import tqdmfrom pathlib import Pathimport osfrom modular_cnn import ModularCNN, make_layersfrom torchsummary import summaryfrom torchutils import pickle_loader, DatasetFolderWithPaths, \    save_checkpoint, RandomHorizontalFlip, Normalize, ToTensorfrom utils import get_num_correct, get_mistakes, get_train_val_idx, online_mean_and_std, calc_accuracy, save_plot_clip_framestorch.backends.cudnn.benchmark=Trueprint('CUDA available:', torch.cuda.is_available())print('CUDA enabled:', torch.backends.cudnn.enabled)# torch.cuda.empty_cache()log_data = FalseHP1 = {"learning_rate": 0.00001               ,"n_epochs": 80               ,"batch_size": 256               ,"num_workers": 6               ,"normalized_data": True               ,"stratified": False               ,"horizontal_flip": False               ,"max_frames": 10               ,"random_seed": 42               ,"flip_prob": 0.5               ,"dataset": "cont3frame_steps"               ,"resolution": 100               ,"adaptive_pool": (5, 7, 7)               ,"features": [8,8,"M",8,8,"M",32,32,32,"M",64,64,"M"]               ,"classifier": [0.5, 400, 0.5, 200, 0.5, 100]                }for hyper_params in [HP1]:    model = ModularCNN(make_layers(hyper_params["features"], batch_norm=True), classifier = hyper_params["classifier"], adaptive_pool=hyper_params["adaptive_pool"])    if torch.cuda.is_available():        model = model.cuda()    # Log number of parameters    hyper_params['trainable_params'] = sum(p.numel() for p in model.parameters())    print('N_trainable_params:', hyper_params['trainable_params'])    data_transforms = transforms.Compose([        ToTensor()        ,Normalize(0.213303, 0.21379)        # ,RandomHorizontalFlip(hyper_params["flip_prob"])    ])    # ROOT_PATH = str("/home/ido/data/" + hyper_params['dataset'])    ROOT_PATH = str('/Users/idofarhi/Documents/Thesis/Data/frames/' + hyper_params['dataset'])    master_data_set = DatasetFolderWithPaths(ROOT_PATH                                    , transform = data_transforms                                    , loader = partial(pickle_loader, min_frames = hyper_params['max_frames'])                                    , extensions = '.pickle'                                    )    train_idx, val_idx = get_train_val_idx(master_data_set, random_state = hyper_params['random_seed'], test_size = 0.2)    train_set = torch.utils.data.Subset(master_data_set, train_idx)    val_set = torch.utils.data.Subset(master_data_set, val_idx)    train_loader = torch.utils.data.DataLoader(train_set                                         , batch_size=hyper_params['batch_size']                                         , shuffle=True                                         # ,batch_sampler =  # TODO: add stratified sampling                                         , num_workers=hyper_params['num_workers']                                         , drop_last=False                                         )    # online_mean_and_std(train_loader)    val_loader = torch.utils.data.DataLoader(val_set                                         , batch_size=hyper_params['batch_size']                                         , shuffle=True                                         # ,batch_sampler =  # TODO: add stratified sampling                                         , num_workers=hyper_params['num_workers']                                         , drop_last=False                                         )    optimizer = optim.Adam(model.parameters(), lr=hyper_params['learning_rate'])    criterion = nn.CrossEntropyLoss()    log_number_train = log_number_val = 0    if log_data:        # Comet ML experiment        experiment = Experiment(api_key="BEnSW6NdCjUCZsWIto0yhxts1" ,project_name="thesis" ,workspace="idodox")        experiment.log_parameters(hyper_params)    summary(model, (1, hyper_params["max_frames"], hyper_params["resolution"], hyper_params["resolution"]))    if torch.cuda.device_count() > 1:      print("Let's use", torch.cuda.device_count(), "GPUs!")      # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs      model = nn.DataParallel(model)    for epoch in tqdm(range(hyper_params["n_epochs"])):        total_train_loss = 0        total_train_correct = 0        incorrect_classifications_train = []        incorrect_classifications_val = []        epoch_classifications_train = []        epoch_classifications_val = []        model.train()        for batch_number, (images, labels, paths) in enumerate(train_loader):            # for i, (image, label, path) in enumerate(zip(images, labels, paths)):            #     save_plot_clip_frames(image, label, path, added_info_to_path = epoch)            images = torch.unsqueeze(images, 1).double()  # added channel dimensions (grayscale)            labels = labels.long()            if torch.cuda.is_available():                images, labels = images.cuda(), labels.cuda()            optimizer.zero_grad() # Whenever pytorch calculates gradients it always adds it to whatever it has, so we need to reset it each batch.            preds = model(images) # Pass Batch            loss = criterion(preds, labels) # Calculate Loss            total_train_loss += loss.item()            loss.backward() # Calculate Gradients - the gradient is the direction we need to move towards the loss function minimum (LR will tell us how far to step)            optimizer.step() # Update Weights - the optimizer is able to update the weights because we passed it the weights as an argument in line 4.            num_correct = get_num_correct(preds, labels)            total_train_correct += num_correct            if log_data:                experiment.log_metric("Train batch accuracy", num_correct/len(labels)*100, step = log_number_train)                experiment.log_metric("Avg train batch loss", loss.item()/len(labels), step = log_number_train)            log_number_train += 1            # print('Train: Batch number:', batch_number, 'Num correct:', num_correct, 'Accuracy:', "{:.2%}".format(num_correct/len(labels)), 'Loss:', loss.item())            incorrect_classifications_train.append(get_mistakes(preds, labels, paths))            for prediction in zip(preds, labels, paths):                epoch_classifications_train.append(prediction)        epoch_accuracy = calc_accuracy(epoch_classifications_train)        if log_data:            experiment.log_metric("Train epoch accuracy", epoch_accuracy, step = epoch)            experiment.log_metric("Avg train epoch loss", total_train_loss/len(epoch_classifications_train), step = epoch)        print('Train: Epoch:', epoch, 'num correct:', total_train_correct, 'Accuracy:', str(epoch_accuracy) + '%', 'Batch loss:', total_train_loss)        total_val_loss = 0        total_val_correct = 0        best_val_acc = 0        model.eval()        with torch.no_grad():            for batch_number, (images, labels, paths) in enumerate(val_loader):                images = torch.unsqueeze(images, 1).double()  # added channel dimensions (grayscale)                labels = labels.long()                if torch.cuda.is_available():                    images, labels = images.cuda(), labels.cuda()                preds = model(images)  # Pass Batch                loss = criterion(preds, labels)  # Calculate Loss                total_val_loss += loss.item()                num_correct = get_num_correct(preds, labels)                total_val_correct += num_correct                if log_data:                    experiment.log_metric("Val batch accuracy", num_correct / len(labels) * 100, step=log_number_val)                    experiment.log_metric("Avg val batch loss", loss.item()/len(labels), step=log_number_val)                log_number_val += 1                # print('Val: Batch number:', batch_number, 'Num correct:', num_correct, 'Accuracy:', "{:.2%}".format(num_correct / len(labels)), 'Loss:', loss.item())                # print_mistakes(preds, labels, paths)                incorrect_classifications_val.append(get_mistakes(preds, labels, paths))                for prediction in zip(preds, labels, paths):                    epoch_classifications_val.append(prediction)            epoch_accuracy = calc_accuracy(epoch_classifications_val)            if log_data:                experiment.log_metric("Val epoch accuracy", epoch_accuracy, step=epoch)                experiment.log_metric("Avg val epoch loss", total_val_loss/len(epoch_classifications_val), step=epoch)            print('Val Epoch:', epoch, 'num correct:', total_val_correct, 'Accuracy:', str(epoch_accuracy) + '%' , 'Batch loss:', total_val_loss)        if epoch >= hyper_params['n_epochs']-1:            print('TRAIN MISCLASSIFICATIONS:')            print(incorrect_classifications_train)            print('TEST MISCLASSIFICATIONS:')            print(incorrect_classifications_val)        is_best = epoch_accuracy > best_val_acc        best_val_acc = max(epoch_accuracy, best_val_acc)        save_checkpoint({            'epoch': epoch + 1,            'state_dict': model.state_dict(),            'best_acc1': best_val_acc,            'optimizer': optimizer.state_dict(),        }, is_best)print("Saving model...")torch.save(model.state_dict(), os.getcwd() + "/model.pt")